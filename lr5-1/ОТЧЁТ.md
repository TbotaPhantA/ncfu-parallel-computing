# ОТЧЕТ
## По лабораторной работе 5: Операции с группами процессов и коммуникаторами. Двумерная декомпозиция матрицы.

### Сведения о студенте
- **Дата:** 09.10.2025 
- **Семестр:** 1
- **Группа:** ПИН-м-о-25-1
- **Дисциплина:** Параллельные вычисления 
- **Студент:** Джабраилов Тимур Султанович

---

## 1. Цель работы
Освоить технику работы с группами процессов и коммуникаторами в MPI. Реализовать параллельный алгоритм умножения матрицы на вектор с двумерным разбиением матрицы на блоки. Исследовать эффективность нового подхода по сравнению с предыдущими реализациями.

## 2. Теоретическая часть
### 2.1. Основные понятия и алгоритмы
Двумерная декомпозиция данных: Разбиение матрицы на блоки по строкам и столбцам
Коммуникаторы и группы процессов: Создание и использование дополнительных
коммуникаторов (Split, Create)
Коллективные операции в подгруппах: Особенности использования Scatterv, Bcast, Reduce в
созданных коммуникаторах
Оптимизация коммуникаций: Снижение объема передаваемых данных за счет двумерной
декомпозиции

### 2.2. Используемые функции MPI
- Get_rank
- Get_size
- Bcast
- Wtime
- Scatterv
- Scatter
- Gatherv
- Reduce
- Allgatherv
- Reduce_scatter
- Allreduce
- Split
- Create
- Get_group

## 3. Практическая реализация
### 3.1. Структура программы
- Двумерная блочная декомпозиция: A - блоки M_part * N_part; вектор x разбит по столбцам (части x_part), результат b - по строкам (части b_part).
- Процессы организованы в квадратную сетку num_row * num_col (num_row = num_col = sqrt(numprocs)).
- Два подкоммуникатора: comm_row (процессы одной строке) и comm_col (процессы одного столбца).
- Поток выполнения: rank-0 читает N,M, матрицу A и вектор x, раздаёт блоки A_part/x_part через Scatterv/временные группы; затем каждый процесс вычисляет локально b_part_temp = A_part * x_part; по строкам выполняется Reduce(SUM) для получения b_part; по столбцам - Gatherv на rank-0 для сборки полного b.

### 3.2. Ключевые особенности реализации
- Подкоммуникаторы (comm_row/comm_col) - изолируют коммуникации: x_part распространяется/броадкастится по столбцам, суммирование локальных вкладов A_part @ x_part происходит внутри строк - минимизируется глобальный трафик.
- Нерегулярное разбиение (N/M не делятся ровно) - решено через rcounts/displs и Scatterv/Gatherv для корректных размеров блоков.
- Сборка блоков A на rank-0 - rank-0 формирует промежуточные буферы по блочным строкам (a_temp) и создаёт временные группы/коммуникаторы (group.Range_incl + comm.Create) для корректной Scatterv только в процессы соответствующего блочного ряда.
- Минимизация корневых операций - явные корни в строках/столбцах (первые процессы) предотвращают конфликт root-параметров и избыточные передачи.

### 3.3. Инструкция по запуску
```bash
# Пример команды для запуска
mpiexec -n 4 python src/main.py
mpiexec -n 9 python src/main.py
mpiexec -n 16 python src/main.py
```

## 4. Экспериментальная часть
### 4.1. Тестовые данные
N = 100, M = 80

### 4.2. Методика измерений
- Intel Core i7 12700H: 6P + 8E ядра (14 ядер)
- DDR4 32GB оперативной памяти
- 5 запусков

### 4.3. Результаты измерений
#### Таблица 1. Время выполнения (секунды)
|numprocs|N  |M  |time                |
|--------|---|---|--------------------|
|4       |100|80 |0.007864699990022928|
|9       |100|80 |0.021679700003005564|
|16      |100|80 |0.010086100024636835|

## 7. Ответы на контрольные вопросы
1. В чем основное преимущество двумерной декомпозиции данных перед одномерной с точки зрения объема передаваемых данных?
> Передаются только части векторов, а не полные копии. Объём данных на процесс уменьшается.

2. Объясните принцип работы функции MPI.Split. Что такое color и key?
> color определяет группу, key - порядок в ней. Процессы с одинаковым color создают новый коммуникатор.

3. Почему при двумерной декомпозиции для равномерного распределения данных желательно использовать число процессов, являющееся полным квадратом?
> Чтобы создать равномерную сетку P_row * P_col и упростить разбиение на одинаковые блоки.

4. Опишите процедуру распределения блоков матрицы с использованием временных коммуникаторов. В чем ее сложность?
> Двойной Scatter: сначала по строкам/столбцам, затем внутри них. Сложность - в расчёте смещений и управлении буферами.

5. Как организована редукция частичных результатов при умножении матрицы на вектор в двумерной декомпозиции и почему именно так?
> Редукция по строкам для суммирования частичных результатов. Это локализует коммуникации.

6. Каким образом в модифицированном МСГ удалось избежать операций с векторами полной длины N?
> Использованием локальных коллективов в подкоммуникаторах вместо глобальных операций.

7. На каких процессах сосредоточена работа с векторами x и p (длины N), а на каких — с векторами b и Ax (длины M) в новой реализации?
> x / p - в столбцовых коммуникаторах (длина N), b / Ax - в строковых (длина M).

8. В каких сценариях (соотношение N и M) двумерная декомпозиция показывает наибольшую эффективность и почему?
> При больших и сравнимых N и M, и большом числе процессов. Т.к. у каждого процессам не должен быть весь вектор, а лишь его кусок.

9. Каковы основные накладные расходы у реализации с двумерной декомпозицией
> Создание коммуникаторов, сложная логика распределения данных, дополнительные коллективные операции.

10. Какие дальнейшие шаги по оптимизации алгоритма вы можете предложить?
> Оптимальный выбор сетки процессов 


## 8. Заключение
### 8.1. Выводы
В ходе данной части лаборатрной работы были получены навыки написания более алгоритма умножения матицы на вектор, который позволяет распараллелить не только операции над строками, но и над стобцами.

## 9. Приложения
### 9.1. Исходный код
```python
import csv
import os
from mpi4py import MPI
from numpy import empty, array, int32, float64, zeros, arange, dot, sqrt, hstack
from matplotlib.pyplot import style, figure, axes, show
from threadpoolctl import threadpool_limits

comm = MPI.COMM_WORLD
numprocs = comm.Get_size()
rank = comm.Get_rank()

prefixPath = './data/'
inPath = prefixPath + 'in.dat'
aDataPath = prefixPath + 'AData.dat'
xDataPath = prefixPath + 'xData.dat'

with threadpool_limits(limits=1):
  if (rank == 0):
      t0 = MPI.Wtime()
  else:
      t0 = None

  if rank == 0 :
      f1 = open(inPath, 'r')
      N = array(int32(f1.readline()))
      M = array(int32(f1.readline()))
      f1.close()
  else :
      N = array(0, dtype=int32)

  comm.Bcast([N, 1, MPI.INT], root=0)

  num_col = num_row = int32(sqrt(numprocs))

  def auxiliary_arrays_determination(M, num) : 
      ave, res = divmod(M, num)
      rcounts = empty(num, dtype=int32)
      displs = empty(num, dtype=int32)
      for k in range(0, num) : 
          if k < res :
              rcounts[k] = ave + 1
          else :
              rcounts[k] = ave
          if k == 0 :
              displs[k] = 0
          else :
              displs[k] = displs[k-1] + rcounts[k-1]   
      return rcounts, displs

  if rank == 0 :
      rcounts_M, displs_M = auxiliary_arrays_determination(M, num_row)
      rcounts_N, displs_N = auxiliary_arrays_determination(N, num_col)
  else :
      rcounts_M = None; displs_M = None
      rcounts_N = None; displs_N = None

  M_part = array(0, dtype=int32); N_part = array(0, dtype=int32)

  comm_col = comm.Split(rank % num_col, rank)
  comm_row = comm.Split(rank // num_col, rank)

  if rank in range(num_col):
      comm_row.Scatter([rcounts_N, 1, MPI.INT], 
                      [N_part, 1, MPI.INT], root=0) 
  if rank in range(0, numprocs, num_col) :
      comm_col.Scatter([rcounts_M, 1, MPI.INT], 
                      [M_part, 1, MPI.INT], root=0) 

  comm_col.Bcast([N_part, 1, MPI.INT], root=0)
  comm_row.Bcast([M_part, 1, MPI.INT], root=0)  

  A_part = empty((M_part , N_part), dtype=float64)

  group = comm.Get_group()

  if rank == 0 :
      f2 = open(aDataPath, 'r')
      for m in range(num_row) :
          a_temp = empty(rcounts_M[m]*N, dtype=float64)
          for j in range(rcounts_M[m]) :
              for n in range(num_col) :
                  for i in range(rcounts_N[n]) :
                      a_temp[rcounts_M[m]*displs_N[n] + j*rcounts_N[n] + i] = float64(f2.readline())
          if m == 0 :
              comm_row.Scatterv([a_temp, rcounts_M[m]*rcounts_N, rcounts_M[m]*displs_N, MPI.DOUBLE], 
                                [A_part, M_part*N_part, MPI.DOUBLE], root=0)
          else :
              group_temp = group.Range_incl([(0,0,1), (m*num_col,(m+1)*num_col-1,1)]) 
              comm_temp = comm.Create(group_temp)
              rcounts_N_temp = hstack((array(0, dtype=int32), rcounts_N))
              displs_N_temp = hstack((array(0, dtype=int32), displs_N))
              comm_temp.Scatterv([a_temp, rcounts_M[m]*rcounts_N_temp, rcounts_M[m]*displs_N_temp, MPI.DOUBLE], 
                                [empty(0, dtype=float64), 0, MPI.DOUBLE], root=0)
              group_temp.Free(); comm_temp.Free()
      f2.close()
  else :
      if rank in range(num_col) :
          comm_row.Scatterv([None, None, None, None], 
                            [A_part, M_part*N_part, MPI.DOUBLE], root=0)
      for m in range(1, num_row) :
          group_temp = group.Range_incl([(0,0,1), (m*num_col,(m+1)*num_col-1,1)])
          comm_temp = comm.Create(group_temp)
          if rank in range(m*num_col, (m+1)*num_col) :
              comm_temp.Scatterv([None, None, None, None], 
                                [A_part, M_part*N_part, MPI.DOUBLE], root=0)
              comm_temp.Free()
          group_temp.Free()
      
  if rank == 0 :
      x = empty(M, dtype=float64)
      f3 = open(xDataPath, 'r')
      for j in range(M) :
          x[j] = float64(f3.readline())
      f3.close()
  else:
      x = None

  x_part = empty(N_part, dtype=float64)

  if rank in range(num_col):
    comm_row.Scatterv([x, rcounts_N, displs_N, MPI.DOUBLE], [x_part, N_part, MPI.DOUBLE], root=0)

  comm_col.Bcast([x_part, N_part, MPI.DOUBLE], root=0)

  b_part_temp = dot(A_part, x_part)

  b_part = empty(M_part, dtype=float64)

  comm_row.Reduce([b_part_temp, M_part, MPI.DOUBLE], [b_part, M_part, MPI.DOUBLE], op=MPI.SUM, root=0)

  if rank == 0:
      b = empty(M, dtype=float64)
  else:
      b = None

  if rank in range(0, numprocs, num_col):
    comm_col.Gatherv([b_part, M_part, MPI.DOUBLE], [b, rcounts_M, displs_M, MPI.DOUBLE], root=0)

  if rank == 0:
      t1 = MPI.Wtime()
      elapsed = t1 - t0
      csv_file = "mat_by_vec.csv"
      need_header = not os.path.exists(csv_file)
      with open(csv_file, "a", newline="") as f:
          writer = csv.writer(f)
          if need_header:
              writer.writerow(["numprocs", "N", "M", "time"])
          writer.writerow([numprocs, N, M, elapsed])
      print(f"nprocs={numprocs}, time={elapsed:.6f} s (written to {csv_file})")
```

### 9.2. Используемые библиотеки и версии
- Python 3.8+
- mpi4py 3.1.+
- NumPy 1.21.+
- OpenMPI 4.1.+
- matplotlib 3.10+

### 9.3. Рекомендуемая литература
Фундаментальные исследования (с аннотациями):
1. Gropp, W., Lusk, E., & Thakur, R. (1999). Using MPI-2: Advanced Features of the Message-Passing
Interface. MIT Press.
Аннотация: Классическое руководство по расширенным возможностям MPI-2. Содержит
детальное описание работы с группами процессов, коммуникаторами и односторонними
коммуникациями, что является теоретической основой для данной лабораторной работы. В
книге подробно разбираются функции MPI_Comm_split и MPI_Comm_create.
2. Thakur, R., Rabenseifner, R., & Gropp, W. (2005). Optimization of Collective Communication
Operations in MPICH. International Journal of High Performance Computing Applications.
Аннотация: Статья глубоко исследует внутренние механизмы и оптимизацию
коллективных операций в одной из самых популярных реализаций MPI — MPICH. Знание
0502_lab_Операции с группами процессов и коммуникаторами. Двумерная декомпозиция матрицы.md
этих принципов помогает понять, почему использование специализированных
коммуникаторов (как в данной работе) может значительно повысить производительность.
3. Barnett, M., Gupta, S., Payne, D. G., & van de Geijn, R. (1993). Broadcasting on Meshes with
Wormhole Routing. Journal of Parallel and Distributed Computing.
Аннотация: Фундаментальное исследование, анализирующее эффективность
коммуникационных операций на сеточных топологиях. Работа закладывает теоретический
базис для понимания того, почему двумерная декомпозиция и соответствующие ей
коммуникационные паттерны могут быть оптимальными для многих линейноалгебраических операций на современных суперкомпьютерных архитектурах.
Практические руководства (с аннотациями):
1. MPI Forum. (2021). MPI: A Message-Passing Interface Standard. Version 4.0.
Аннотация: Официальная спецификация стандарта MPI. Является первоисточником и
исчерпывающим справочником по всем функциям MPI, их аргументам и поведению.
Незаменима для точного понимания семантики используемых в работе функций, таких как
Scatterv, Reduce и Split.
2. Pacheco, P. (2011). An Introduction to Parallel Programming. Morgan Kaufmann.
Аннотация: Отличное практическое введение в параллельное программирование,
включая обширный раздел по MPI. Книга содержит множество примеров и упражнений,
которые помогают закрепить концепции на практике, и служит хорошим дополнением к
лекционному материалу.
3. Gropp, W., Hoefler, T., Thakur, R., & Lusk, E. (2014). Using Advanced MPI: Modern Features of the
Message-Passing Interface. MIT Press.
Аннотация: Прямое продолжение классической книги по MPI-2, фокусирующееся на
современных возможностях. Содержит разделы, посвященные топологиям виртуальных
коммуникаторов и гибридному программированию, что является логичным следующим
шагом после освоения материала данной лабораторной работы.


*Отчет подготовлен в рамках курса "Параллельные вычисления"*
