# ОТЧЕТ
## По лабораторной работе 6: Виртуальные топологии в MPI. Оптимизация коммуникаций

### Сведения о студенте
- **Дата:** 11.10.2025 
- **Семестр:** 1
- **Группа:** ПИН-м-о-25-1
- **Дисциплина:** Параллельные вычисления 
- **Студент:** Джабраилов Тимур Султанович

---

## 1. Цель работы
Освоить технику создания и использования виртуальных топологий в MPI. Изучить функции Create_cart, Shift и Sendrecv_replace для оптимизации коммуникационных операций в параллельных алгоритмах. Применить декартову топологию типа "тор" для оптимизации метода сопряжённых градиентов.

## 2. Теоретическая часть
### 2.1. Основные понятия и алгоритмы
Декартовы топологии: Создание двумерной сетки процессов, периодические границы
Оптимизация коммуникаций: Замена коллективных операций на топологическиориентированные обмены

### 2.2. Используемые функции MPI
- Get_rank
- Get_size
- Create_cart
- Shift
- Sendrecv_replace

## 3. Практическая реализация
### 3.1. Структура программы
Программа реализует параллельные вычисления с использованием декартовой топологии MPI. Архитектура представляет собой:
- 2D сетку процессов размером 2×4 (всего 8 процессов)
- Каждый процесс работает с локальным массивом данных
- Данные циркулируют по кольцевой топологии в пределах каждой строки сетки
- Выполняется редукция с циклическим сдвигом для суммирования данных

### 3.2. Ключевые особенности реализации
Алгоритм циклического сдвига
```python
for n in range(num_col - 1):
    comm_cart.Sendrecv_replace([a, 2, MPI.INT], dest=neighbour_right, ...)
    sum = sum + a
```
- Sendrecv_replace - атомарная операция отправки/получения с заменой буфера
- Данные циркулируют по строке, каждый процесс суммирует полученные массивы
- После (num_col-1) итераций каждый процесс имеет сумму всех данных строки

### 3.3. Инструкция по запуску
```bash
# Пример команды для запуска
mpiexec -n 8 python src/main.py
```

## 4. Экспериментальная часть
### 4.1. Тестовые данные
N = 4, M = 2

### 4.2. Методика измерений
- Intel Core i7 12700H: 6P + 8E ядра (14 ядер)
- DDR4 32GB оперативной памяти
- 1 запуск

### 4.3. Результат выполнения
Process 0 has sum=[10 14]
Process 1 has sum=[10 14]
Process 5 has sum=[20 28]
Process 3 has sum=[10 14]
Process 4 has sum=[20 28]
Process 6 has sum=[20 28]
Process 2 has sum=[10 14]
Process 7 has sum=[20 28]

## 5. Анализ результата
Программа выполняет построчную редукцию суммы в 2D сетке процессов. Каждый процесс начинает с локального массива, затем данные циркулируют по кольцу в каждой строке, и каждый процесс накапливает сумму всех массивов в своей строке. Это частичный аналог Allreduce, но с важными отличиями: в отличие от классического Allreduce, где все процессы получают одинаковый результат полной редукции, здесь редукция выполняется только в пределах строк декартовой сетки, поэтому процессы из разных строк получают разные результаты.

## 9. Приложения
### 9.1. Исходный код
```python
from mpi4py import MPI
from numpy import array, int32

comm = MPI.COMM_WORLD
numprocs = comm.Get_size()
rank = comm.Get_rank()

num_row = 2
num_col = 4

comm_cart = comm.Create_cart(dims=(num_row, num_col), periods=(True, True), reorder=True)

rank_cart = comm_cart.Get_rank()

neighbour_up, neighbour_down = comm_cart.Shift(direction=0, disp=1)
neighbour_left, neighbour_right = comm_cart.Shift(direction=1, disp=1)

a = array([(rank_cart % num_col + 1 + i) * 2 ** (rank_cart // num_col) for i in range(2)], dtype=int32)

sum = a.copy()
for n in range(num_col - 1):
    comm_cart.Sendrecv_replace([a, 2, MPI.INT], dest=neighbour_right, sendtag=0, source=neighbour_left, recvtag=MPI.ANY_TAG, status=None)

    sum = sum + a

print('Process {} has sum={}'.format(rank_cart, sum))
```

### 9.2. Используемые библиотеки и версии
- Python 3.8+
- mpi4py 3.1.+
- NumPy 1.21.+
- OpenMPI 4.1.+
- matplotlib 3.10+

---

*Отчет подготовлен в рамках курса "Параллельные вычисления"*
