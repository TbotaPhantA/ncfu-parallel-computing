# ОТЧЕТ
## По лабораторной работе 4: Теория параллельных вычислений

### Сведения о студенте
**Дата:** 09.10.2025 
**Семестр:** 1
**Группа:** ПИН-м-о-25-1
**Дисциплина:** Параллельные вычисления 
**Студент:** Джабраилов Тимур Султанович

---

## 1. Цель работы
Экспериментально исследовать эффективность и масштабируемость параллельных программ на примере реализованных ранее алгоритмов. Освоить методику измерения и анализа производительности параллельных вычислений. Проверить на практике закон Амдала и изучить факторы, влияющие на масштабируемость

## 2. Теоретическая часть
### 2.1. Основные понятия и алгоритмы
Закон Амдала: Ограничения на ускорение параллельных программ
Метрики производительности: Ускорение (Speedup), эффективность (Efficiency)
Типы масштабируемости: Сильная (strong scaling) и слабая (weak scaling)
Факторы влияния: Накладные расходы на коммуникацию, балансировка нагрузки,
характеристики вычислительной системы

### 2.2. Используемые функции MPI
Тестируются код из предыдущих лабораторных работ
- Get_rank
- Get_size
- Bcast
- Wtime
- Scatterv
- Scatter
- Gatherv
- Send
- Recv
- Reduce
- Allgatherv
- Reduce_scatter
- Allreduce

## 3. Практическая реализация
### 3.1. Структура программы
В рамках данной лабораторной не пишутся новые программы, а лишь проводится анализ существующих

### 3.2. Ключевые особенности реализации
В рамках данной лабораторной не пишутся новые программы, а лишь проводится анализ существующих

### 3.3. Инструкция по запуску
В рамках данной лабораторной не пишутся новые программы, а лишь проводится анализ существующих

## 4. Экспериментальная часть
### 4.1. Тестовые данные
Набор A: N = 20, M = 2 000 000
Набор B: N = 50, M = 800 000
Набор C: N = 100, M = 200 000

### 4.2. Методика измерений
- Intel Core i7 12700H: 6P + 8E ядра (14 ядер)
- DDR4 32GB оперативной памяти

### 4.3. Результаты измерений
#### Таблица 1. lr1 (умножение матрицы на вектор)
|numprocs|N  |M      |time |
|--------|---|-------|-----|
|2       |20 |2000000|14.06|
|4       |20 |2000000|13.22|
|8       |20 |2000000|13.09|
|11      |20 |2000000|12.86|
|2       |50 |800000 |13.19|
|4       |50 |800000 |12.6 |
|8       |50 |800000 |12.24|
|11      |50 |800000 |12.1 |
|2       |100|200000 |6.87 |
|4       |100|200000 |6.11 |
|8       |100|200000 |6.12 |
|11      |100|200000 |6.16 |

#### Таблица 2. lr2 (умножение транспонированой матрицы на вектор)
|numprocs|N  |M      |time |
|--------|---|-------|-----|
|2       |2000000|20     |11.98|
|4       |2000000|20     |12.01|
|8       |2000000|20     |12.85|
|11      |2000000|20     |12.21|
|2       |800000|50     |11.92|
|4       |800000|50     |12.16|
|8       |800000|50     |12.65|
|11      |800000|50     |12.32|
|2       |200000|100    |6.27 |
|4       |200000|100    |6.31 |
|8       |200000|100    |6.6  |
|11      |200000|100    |6.32 |

#### Таблица 3. lr3-1 (решение СЛАУ параллельное)
|numprocs|N  |M      |time |
|--------|---|-------|-----|
|2       |20 |2000000|15.056232099974295|
|4       |20 |2000000|15.549593500007177|
|8       |20 |2000000|14.530880900012562|
|11      |20 |2000000|14.988982400012901|
|2       |50 |800000 |16.218943799991393|
|4       |50 |800000 |14.812278299999889|
|8       |50 |800000 |14.864018299995223|
|11      |50 |800000 |14.510582899994915|
|2       |100|200000 |8.093111900001531|
|4       |100|200000 |7.909049699985189|
|8       |100|200000 |7.826438699994469|
|11      |100|200000 |7.67567450000206|

#### Таблица 4. lr3-2 (решение СЛАУ упрощённое)
|numprocs|N  |M      |time |
|--------|---|-------|-----|
|2       |20 |2000000|15.672760999994352|
|4       |20 |2000000|15.238140200002817|
|8       |20 |2000000|14.99483360000886|
|11      |20 |2000000|14.602780599991092|
|2       |50 |800000 |15.130303400015691|
|4       |50 |800000 |14.508052300021518|
|8       |50 |800000 |14.480089200020302|
|11      |50 |800000 |14.40788870002143|
|2       |100|200000 |8.4821735000005|
|4       |100|200000 |7.32206970002153|
|8       |100|200000 |6.985806099983165|
|11      |100|200000 |6.970212299987907|

## 5. Анализ результатов и визуализация результатов
### 5.1 Ускорение
![image](/lr4/images/ускорение1.png)

в измерениях при переходе от 2 → 4 → 8 → 11 процессов наблюдается небольшой выигрыш в реальном времени: коммуникационные накладные и/или последовательные части программы заметно ограничивают ускорение. Оптимальное число процессов - 11.

![image](/lr4/images/ускорение2.png)

в измерениях при переходе от 2 → 4 → 8 → 11 процессов наблюдается падение ускорения, быстрее всего себя показывает однопоточная программа. Оптимальное число процессов - 2.

![image](/lr4/images/ускорение3.png)

в измерениях при переходе от 2 → 4 → 8 → 11 процессов наблюдается небольшой выигрыш в реальном времени: коммуникационные накладные и/или последовательные части программы заметно ограничивают ускорение. Оптимальное число процессов - 11.

![image](/lr4/images/ускорение4.png)

в измерениях при переходе от 2 → 4 → 8 → 11 процессов наблюдается небольшой выигрыш в реальном времени: коммуникационные накладные и/или последовательные части программы заметно ограничивают ускорение. Оптимальное число процессов - 11.

### 5.2 Эффективность

![image](/lr4/images/эффективность1.png)

По данному результату можно понять, что эффективность на всех этапах падает для всех наборов данных. С алгоримом есть явные проблемы, и программы, и алгоритмы требуют определённого усовершенствования. Однако с увеличением кол-ва процессов падение эффектвности.

![image](/lr4/images/эффективность2.png)

По данному результату можно понять, что эффективность на всех этапах падает для всех наборов данных. С алгоримом есть явные проблемы, и программы, и алгоритмы требуют определённого усовершенствования. Однако с увеличением кол-ва процессов падение эффектвности.


![image](/lr4/images/эффективность3.png)

По данному результату можно понять, что эффективность на всех этапах падает для всех наборов данных. С алгоримом есть явные проблемы, и программы, и алгоритмы требуют определённого усовершенствования. Однако с увеличением кол-ва процессов падение эффектвности.


![image](/lr4/images/эффективность4.png)

По данному результату можно понять, что эффективность на всех этапах падает для всех наборов данных. С алгоримом есть явные проблемы, и программы, и алгоритмы требуют определённого усовершенствования. Однако с увеличением кол-ва процессов падение эффектвности.


### 5.3 Сравнение производительности 3-1 и 3-2

![image](/lr4/images/скорость_выполнения3.png)

![image](/lr4/images/скорость_выполнения4.png)

В среднем по наборам данных упрощённая версия (Табл.4) показывает меньшее время для большинства сочетаний N,M. Это согласуется с идеей упрощённой версии: меньше коммуникаций/меньше коллективных операций - лучшее время.


### 5.4 Слабая масштабируемость

![image](/lr4/images/слабая_масштабируемость1.png)

Во всех графиках при росте числа процессов видно изменение, тогда как при идеальной слабой масштабируемости эта величина должна оставаться близкой к константе. Это значит, что добавление процессов изменяет накладные расходы.

![image](/lr4/images/слабая_масштабируемость2.png)

Во всех графиках при росте числа процессов видно увеличение. Это значит, что добавление процессов изменяет накладные расходы относительно объема данных в большую сторону. 

![image](/lr4/images/слабая_масштабируемость3.png)

разные пары (N,M) дают заметно разные результаты - следовательно, важны не только общий объём работы, но и способ её разбиения (строки/столбцы, размер передаваемых сообщений, кэш-локальность).

![image](/lr4/images/слабая_масштабируемость4.png)

Упрощённая версия (Таблица 4) в целом показывает более низкие результаты и более стабильное поведение, чем тяжёлый параллельный алгоритм.

# 5.5 Какие MPI-функции являются самыми проблемными и влияние N
- Allgatherv
- Reduce_scatter
- Allreduce
Из-за пересылки массивов большой длинны всем процессам. Поэтому увеличение N можемт приводить к проблемам с производительностью.

# 5.6 Оценка объёма передаваемых данных
Алноритыми рассылки всреднем = 𝑁 ⋅ 8 ⋅ log ⁡ 2 (n) N⋅8⋅log 2 ​ (p) байт, где
N - кол-во элементов в строке
8 - размер float64
log 2 (n) - дерево рассылки по процессам

# 5.7 Как изменение 𝑁 N влияет на время коммуникаций
- рост 𝑁 N прямо увеличит время на эти коллективы (линейно в первом приближении), особенно если сеть узкая или задержки велики.
- Если 𝑁 N очень мал, то узким местом становится не столько объём одного сообщения, сколько количество коллективных вызовов и их синхронизирующий эффект (latency, барьеры, частые маленькие сообщения). То есть даже при малых 𝑁 N частые коллективные операции портят масштабируемость.

# 5.8 Рекомендации по улучшению масштабируемости
- Уменьшить число коллективных операций: заменить Allgatherv/Allreduce там, где можно, на локальные операции + Reduce_scatter / Gather с меньшими объёмами.
- Распараллеливать не только строки, но и столбцы

## 7. Ответы на контрольные вопросы
1. **Закон Амдала:** (S(n)=1/((1-p)+p/n)). Ускорение ограничено последовательной долей (1-p), накладными на синхронизацию, коммуникации и балансом нагрузки.
2. **Сильная vs слабая масштабируемость:** сильная - фикс. задача, уменьшается время; слабая — фикс. работа на процесс, растёт задача (важно для увеличения размера задачи).
3. **Почему эффективность падает:** растут коммуникации, синхронизации и накладные; появляется несбалансированность и конкуренция за ресурсы.
4. **Влияние N и M в CG:** больше M - больше вычислений, больше N - больше глобальных векторов/коллективов (усиление коммуникаций).
5. **Самые «дорогие» MPI-функции:** коллективы — `Allreduce`, `Allgatherv`, `Alltoall`, `Reduce_scatter` (из-за синхронизации и больших объёмов обмена).
6. **Как выбрать оптимальный p:** измерить T(n), вычислить эффективность; выбрать p, где отдача мала (или E падает ниже порога, напр. 0.7–0.8).
7. **Аномалии на многоядерном CPU:** NUMA, кэш контеншн, планировщик ОС, троттлинг, фоновые задачи.
8. **Влияние планировщика кластера:** размещение по узлам, соседние джобы и переподписка влияют на сеть и стабильность таймингов.
9. **Оптимизация коммуникаций:** Распараллеливать не только строки но и столбцы
10. **Главное из «Twelve ways...» :** будьте честны — указывайте конфигурацию, входы, средние/разброс; не выбирайте искусственные кейсы и не скрывайте методику.

## 8. Заключение
### 8.1. Выводы
В ходе данной лабораторной работы были получены навыки анализа ускорения, эффективности и масштабируемости параллельных программ, были выявлены проблемы и "узкие места" который будут решаться в рамках следующих лабораторных работ. 

### 8.2. Проблемы и решения
Программа хуже справляется с большыми N. Нужно постараться избавиться от операций Allreduce, Allgatherv, Alltoall, Reduce_scatter.

### 8.3. Перспективы улучшения
Необходимо распараллеливать не только строки но и столбцы.

## 9. Приложения
### 9.1. Исходный код
В рамках данной лабораторной не пишутся новые программы, а лишь проводится анализ существующих

### 9.2. Используемые библиотеки и версии
- Python 3.8+
- mpi4py 3.1.+
- NumPy 1.21.+
- OpenMPI 4.1.+
- matplotlib 3.10+

### 9.3. Рекомендуемая литература
Фундаментальные исследования:
1. Amdahl, G. M. (1967). Validity of the Single Processor Approach to Achieving Large Scale Computing
Capabilities. AFIPS Conference Proceedings. Том 30.
Аннотация: Классическая работа, в которой впервые был сформулирован "Закон Амдала".
Статья заложила фундамент для теоретического анализа пределов распараллеливания и
является обязательной к прочтению для понимания базовых принципов параллельных
вычислений.
2. Gustafson, J. L. (1988). Reevaluating Amdahl's Law. Communications of the ACM. Том 31, № 5.
Аннотация: Статья, в которой предложен альтернативный взгляд на закон Амдала ("Закон
Густавсона"). Автор доказывает, что при увеличении размера задачи можно достичь почти
линейного ускорения, что более адекватно описывает поведение реальных приложений на
massively parallel системах.
3. Dongarra, J., et al. (2016). The International Exascale Software Project roadmap. International Journal of
High Performance Computing Applications. Том 25, № 1.
Аннотация: Масштабный обзор-дорожная карта, определяющая ключевые проблемы и
направления исследований в области программного обеспечения для экзафлопсных
вычислений. Статья раскрывает современные вызовы, связанные с масштабируемостью,
энергоэффективностью и устойчивостью алгоритмов.
Практические руководства:
1. Gropp, W., Lusk, E., & Skjellum, A. (2014). Using MPI: Portable Parallel Programming with the MessagePassing Interface (3rd ed.). MIT Press.
Аннотация: Фундаментальное практическое руководство по стандарту MPI, являющееся
фактически "библией" для разработчиков параллельных приложений. Книга содержит как
0402_lab_Анализ эффективности и масштабируемости параллельных программ.md
4 / 4
теоретические основы, так и подробные примеры реализации, напрямую связанные с
темами ваших лабораторных работ.
2. Voevodin, V. V., & Zhumatii, S. A. (2019). The Lomonosov Supercomputers and Their Applications. In V.
Voevodin & S. Sobolev (Eds.), Supercomputing. RuSCDays 2019. Lecture Notes in Computer Science, vol
11289. Springer.
Аннотация: Статья, описывающая архитектуру и опыт эксплуатации суперкомпьютера
"Ломоносов-2", на котором проводились замеры производительности для лекции.
Позволяет понять среду выполнения, в которой работают реальные MPI-приложения.
3. The MPI Forum. (2021). MPI: A Message-Passing Interface Standard. Version 4.0.
Аннотация: Официальная спецификация стандарта MPI. Является первоисточником для
понимания семантики всех используемых в курсе функций (Scatterv, Gatherv, Allreduce,
Reduce_scatter и др.). Незаменима для глубокого понимания механизмов работы
библиотеки.

*Отчет подготовлен в рамках курса "Параллельные вычисления"*
