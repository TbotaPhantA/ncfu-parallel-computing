# ОТЧЕТ
## По лабораторной работе 8: Параллелизация явной схемы для одномерного уравнения теплопроводности

### Сведения о студенте
**Дата:** 12.10.2025 
**Семестр:** 1 
**Группа:** ПИН-м-о-25-1 
**Дисциплина:** Параллельные вычисления 
**Студент:** Джабраилов Тимур Султанович

---

## 1. Цель работы
Освоить методы распараллеливания алгоритмов решения уравнений в частных производных на примере явной схемы для одномерного уравнения теплопроводности. Изучить особенности распределения данных и организации коммуникаций между процессами при решении сеточных задач. Сравнить эффективность различных подходов к распараллеливанию.

## 2. Теоретическая часть
### 2.1. Основные понятия и алгоритмы
Рассматривается начально-краевая задача для уравнения параболического типа:
$$ \begin{cases} \varepsilon \frac{\partial^2 u}{\partial x^2} - \frac{\partial u}{\partial t} = -u \frac{\partial u}
{\partial x} - u^3, & x \in (a,b), , t \in (t_0,T], \ u(a,t) = u_{\text{left}}(t), & u(b,t) = u_{\text{right}}(t), , t \in (t_0,T],
\ u(x,t_0) = u_{\text{init}}(x), & x \in [a,b]. \end{cases} $$
Для численного решения используется явная схема:
$$ u_{n}^{m+1} = u_{n}^{m} + \varepsilon \frac{\tau}{h^2} \left( u_{n+1}^{m} - 2u_{n}^{m} + u_{n-1}^{m}
\right) + \frac{\tau}{2h} u_{n}^{m} \left( u_{n+1}^{m} - u_{n-1}^{m} \right) + \tau (u_{n}^{m})^3 $$

### 2.2. Используемые функции MPI
- comm.Get_size()
- comm.Get_rank()
- MPI.Wtime()
- comm.Scatter()
- comm.Scatterv()
- comm.Gatherv()

## 3. Практическая реализация
### 3.1. Структура программы
Программа параллельно решает ту же одномерную КУЧП (аналогично Бюргерсу/реакционно-диффузионному уравнению) с помощью mpi4py: процесс rank=0 инициализирует полную сетку и рассчитывает массивы rcounts/displs для разбиения пространственной сетки между процессами, затем в основном временном цикле корневой процесс с помощью Scatterv раздаёт каждому процессу его локальную часть (с добавленными соседними узлами), каждый процесс выполняет явное разностное обновление для своих внутренних точек, а Gatherv собирает обновлённые куски обратно на корень.

### 3.2. Ключевые особенности реализации
Полная матрица решения хранится только на корневом процессе (экономия памяти), а передача данных реализована через Scatterv/Gatherv

### 3.3. Инструкция по запуску
```bash
# Пример команды для запуска
mpiexec -n 14 python main.py
```

## 4. Экспериментальная часть
### 4.1. Тестовые данные
N = 200; M = 20000

### 4.2. Методика измерений
- Intel Core i7 12700H: 6P + 8E ядра (14 ядер)
- DDR4 32GB оперативной памяти
- 5 запусков

### 4.3. Результаты измерений
#### Таблица 1. Время выполнения (секунды)
|numprocs|N  |M    |time  |
|--------|---|-----|------|
|2       |200|20000|2.1523555000312626|
|4       |200|20000|1.2417560999747366|
|8       |200|20000|1.248762400005944|
|14      |200|20000|1.1489999999757856|

### 5.1 Визуализация резульатата
![image](./images/plot_u_vs_x.png)
Результат расчётов в виде графика x и u

## 9. Приложения
### 9.1. Исходный код
```python
from mpi4py import MPI
from numpy import empty, array, int32, float64, linspace, sin, pi, hstack

comm = MPI.COMM_WORLD
numprocs = comm.Get_size()
rank = comm.Get_rank()

def u_init(x) :
    u_init = sin(3*pi*(x - 1/6))
    return u_init

def u_left(t) :
    u_left = -1.
    return u_left

def u_right(t) :
    u_right = 1.
    return u_right

if rank == 0 : start_time = MPI.Wtime()

a = 0.; b = 1.
t_0 = 0.; T = 6.0
eps = 10**(-1.5)

N = 200; M = 20000

h = (b - a)/N; x = linspace(a, b, N+1)
tau = (T - t_0)/M; t = linspace(t_0, T, M+1)

if rank == 0 :
    ave, res = divmod(N + 1, numprocs)
    rcounts = empty(numprocs, dtype=int32)
    displs = empty(numprocs, dtype=int32)
    for k in range(0, numprocs) : 
        if k < res :
            rcounts[k] = ave + 1
        else :
            rcounts[k] = ave
        if k == 0 :
            displs[k] = 0
        else :
            displs[k] = displs[k-1] + rcounts[k-1]   
else :
    rcounts = None; displs = None
    
N_part = array(0, dtype=int32)

comm.Scatter([rcounts, 1, MPI.INT], [N_part, 1, MPI.INT], root=0) 

if rank == 0 :
    rcounts_from_0 = empty(numprocs, dtype=int32)
    displs_from_0 = empty(numprocs, dtype=int32)
    rcounts_from_0[0] = rcounts[0] + 1
    displs_from_0[0] = 0
    for k in range(1, numprocs-1) :
        rcounts_from_0[k] = rcounts[k] + 2
        displs_from_0[k] = displs[k] - 1
    rcounts_from_0[numprocs-1] = rcounts[numprocs-1] + 1  
    displs_from_0[numprocs-1] = displs[numprocs-1] - 1
else :
    rcounts_from_0 = None; displs_from_0 = None
    
N_part_aux = array(0, dtype=int32)
    
comm.Scatter([rcounts_from_0, 1, MPI.INT], [N_part_aux, 1, MPI.INT], root=0) 

if rank == 0 :
    u = empty((M+1, N+1), dtype=float64)
    for n in range(N + 1) :
        u[0, n] = u_init(x[n]) 
else :
    u = empty((M+1, 0), dtype=float64)
        
u_part = empty(N_part, dtype=float64)
u_part_aux = empty(N_part_aux, dtype=float64)

for m in range(M) :
    
    comm.Scatterv([u[m], rcounts_from_0, displs_from_0, MPI.DOUBLE], 
                  [u_part_aux, N_part_aux, MPI.DOUBLE], root=0)
    
    for n in range(1, N_part_aux - 1) :
        u_part[n-1] = u_part_aux[n] + \
            eps*tau/h**2*(u_part_aux[n+1] - 2*u_part_aux[n] + u_part_aux[n-1]) + \
                tau/(2*h)*u_part_aux[n]*(u_part_aux[n+1] - u_part_aux[n-1]) + \
                    tau*u_part_aux[n]**3
                    
    if rank == 0 :
        u_part = hstack((array(u_left(t[m+1]), dtype=float64), u_part[0:N_part-1]))
    elif rank == numprocs-1 :
        u_part = hstack((u_part[0:N_part-1], array(u_right(t[m+1]), dtype=float64)))
        
    comm.Gatherv([u_part, N_part, MPI.DOUBLE], 
                 [u[m+1], rcounts, displs, MPI.DOUBLE], root=0)

if rank == 0 :
    
    end_time = MPI.Wtime()
    print('N={}, M={}'.format(N, M))
    print('Number of MPI process is {}'.format(numprocs))
    print('Elapsed time is {:.4f} sec.'.format(end_time-start_time))
    
    from numpy import savez
    savez('results_of_calculations', x=x, u=u)
```

### 9.2. Используемые библиотеки и версии
- Python 3.8+
- mpi4py 3.1.+
- NumPy 1.21.+
- OpenMPI 4.1.+

### 9.3. Рекомендуемая литература
Фундаментальные исследования (с аннотациями):
1. Hockney, R. W., & Jesshope, C. R. (1988). Parallel Computers 2: Architecture, Programming and
Algorithms. Adam Hilger.
Аннотация: Классическая работа, посвящённая архитектуре параллельных вычислительных
систем и алгоритмам для них. Содержит глубокий анализ методов распараллеливания сеточных
задач, включая явные и неявные схемы для уравнений в частных производных.
2. Ortega, J. M. (1988). Introduction to Parallel and Vector Solution of Linear Systems. Springer.
Аннотация: Фундаментальное введение в методы решения систем линейных уравнений на
параллельных архитектурах. Особое внимание уделяется алгоритмам для разреженных матриц и
их применению в задачах математической физики.
3. Gustafsson, B. (2008). High Order Difference Methods for Time Dependent PDE. Springer.
Аннотация: Монография, посвящённая численным методам высокого порядка точности для
решения уравнений в частных производных. Содержит анализ устойчивости и сходимости
разностных схем, а также вопросы их распараллеливания.
Практические руководства (с аннотациями):
1. Gropp, W., Lusk, E., & Skjellum, A. (2014). Using MPI: Portable Parallel Programming with the
Message-Passing Interface. MIT Press.
Аннотация: Практическое руководство по программированию с использованием MPI. Содержит
примеры реализации распределённых алгоритмов, включая задачи для УрЧП и методы
организации коммуникаций.
2. Pacheco, P. (2011). An Introduction to Parallel Programming. Morgan Kaufmann.
Аннотация: Учебник, ориентированный на практическое освоение параллельного
программирования. Включает разделы по MPI и примеры распараллеливания сеточных методов.
3. Dongarra, J. J., et al. (1998). Numerical Linear Algebra for High-Performance Computers. SIAM.
Аннотация: Сборник практических рекомендаций по реализации алгоритмов линейной алгебры
на высокопроизводительных системах. Содержит оптимизированные реализации методов для

---

*Отчет подготовлен в рамках курса "Параллельные вычисления"*