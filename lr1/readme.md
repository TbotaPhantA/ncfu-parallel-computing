МИНИCTEPCTBO НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ ФЕДЕРАЛЬНОЕ ГОСУДАРСТВЕННОЕ АВТОНОМНОЕ ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ ВЫСШЕГО ОБРАЗОВАНИЯ

«СЕВЕРО-КАВКАЗСКИЙ ФЕДЕРАЛЬНЫЙ УНИВЕРСИТЕТ»

**Департамент цифровых, робототехнических систем и электроники**

ОТЧЕТ ПО ЛАБОРАТОРНОЙ РАБОТЕ № 1

ДИСЦИПЛИНЫ «Параллельные вычисления»

НА ТЕМУ:

«Параллельное умножение матрицы на вектор с использованием MPI в Python»

**Выполнил:**

студент группы ПИН-м-о-25-1

ФИО Джабраилов Тимур Султанович

Проверил: старший преподаватель департамента цифровых, робототехнических систем и электроники института перспективной инженерии Щёголев А. А.

Ставрополь, 2024

---

### Задание на практическую реализацию:
> Необходимо реализовать программу на Python с использованием mpi4py, которая выполняет
> параллельное умножение матрицы на вектор. Программа должна состоять из нескольких этапов.

#### Этап 1: Последовательная версия (верификация)
> Реализуйте последовательную версию умножения матрицы на вектор (без использования MPI) для проверки правильности работы параллельной версии. Используйте numpy.dot() для финальной проверки.

Для начала необходимо разработать последовательную версию программы, умножающей матрицу на вектор.

![](/lr1/static/1.png)

Рисунок 1 - реализация последовательного вычисления умножения матрицы на вектор

После запуска программы результат записывается в файл:

![](/lr1/static/2.png)

Рисунок 2 - команда для запуска последовательной программы

![](/lr1/static/3.png)

Рисунок 3 - результат выполнения последовательно программы

#### Этап 2: Базовая параллельная версия

> 1. Инициализация MPI: Получите ранг процесса и общее количество процессов.
> 2. Чтение параметров: Процесс 0 считывает из файла in.dat размеры матрицы (M, N). Используйте MPI.Bcast для рассылки этих размеров всем процессам.
> 3. Распределение данных:
> - Процесс 0 считывает всю матрицу A и вектор x из файлов AData.dat и xData.dat.
> - Рассчитайте размер блока данных local_M для каждого процесса. Для простоты предположите, что M без остатка делится на число процессов (size). (Усложнение: см. Этап 4)
> - Используйте цикл с MPI.Send на процессе 0 и MPI.Recv на остальных процессах, чтобы разослать блоки матрицы A_part каждому процессу.
> - Разошлите весь вектор x всем процессам с помощью MPI.Bcast.
> 4. Локальное вычисление: Каждый процесс (включая процесс 0) вычисляет свою часть результата: b_part = numpy.dot(A_part, x).
> 5. Сбор результатов: Используйте цикл с MPI.Recv на процессе 0 и MPI.Send на остальных процессах, чтобы собрать все части b_part в итоговый вектор b.
> 6. Вывод результата: Процесс 0 записывает итоговый вектор b в файл Results_parallel.dat.


Далее необходимо реализовать параллельную реализацию программы с использованием библиотеки mpi4py.

![](/lr1/static/4.png)

Рисунок 4 - реализация последовательного вычисления умножения матрицы на вектор

При выполнении данной программы на 5-ти процесса для матрицы с 80-ю строками также генерируется файл с результирующей результирующим вектором. Можно сравнить его с результатом выполнения последовательной программы, для проверки корректности.

![](/lr1/static/5.png)

Рисунок 5 - команда для запуска параллельно программы

![](/lr1/static/6.png)

Рисунок 6 - сравнение результатов последовательной и параллельной программы


Как можно заметить на рисунке 6, результаты совпадают. Далее необходимо оптимизировать программу с помощью функций Scatterv и Gatherv, а также учесть, что кол-во процессов может не быть кратно кол-ву строк в матрице, для этого перепишем некоторые части программы.

#### Этап 3 и 4: Оптимизация с помощью коллективных операций и Обработка произвольного размера матрицы (повышенная сложность)
> Замените операции точка-точка (Send/Recv) на коллективные операции там, где это возможно
> 1. Для распределения блоков матрицы A используйте MPI.Scatterv.
> 2. Для сбора частей результата b_part используйте MPI.Gatherv.
> 3. Убедитесь, что программа работает так же, как и на предыдущем этапе.
> 
> Модифицируйте программу, чтобы она корректно работала, когда количество строк M не делится без остатка на число процессов.
> - На процессе 0 рассчитайте массив rcounts (количество строк для каждого процесса) и displs (смещения для каждого блока в общей матрице).
> - Используйте эти массивы в функциях MPI.Scatterv и MPI.Gatherv.
> - Пример: Для M=5 и size=3: rcounts = [2, 2, 1], displs = [0, 2, 4].

![](/lr1/static/7.png)

Рисунок 7 - реализация оптимизированного последовательного вычисления умножения матрицы на вектор и учёт возможной не кратности кол-ва процессов кол-ву строк в матрице

Далее необходимо выполнить программу на 7-ми процессах, что не кратно 80-ю строкам матрицы и сравнить результат.

![](/lr1/static/8.png)

Рисунок 8 - команда для запуска оптимизированной параллельной программы

![](/lr1/static/9.png)

Рисунок 9 - сравнение результатов последовательной и оптимизированной параллельной программы

**Вывод**

В ходе данной лабораторной работы было реализованное параллельное вычисление умножения матрицы на вектор с помощью библиотеки mpi4py и проведены эксперименты, подтверждающие правильность работы программы.